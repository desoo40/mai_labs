{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Факультет \"Прикладная математика\" МАИ</h1>\n",
    "<h2>Курс \"Основы Python для анализа данных\"</h2>\n",
    "<h2>Артамонов Игорь Михайлович</h2>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Занятие № 11. Обучение без учителя. PCA. Кластеризация.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общение / вопросы по курсу\n",
    "\n",
    "Платформа для групповой работы Atlassian Confluence факультета \"Прикладная математика\"\n",
    "\n",
    "https://mai.moscow/display/PYTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>Занятие № 11. Обучение без учителя. PCA. Кластеризация.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## virtualenv + Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<Ctrl> + <Alt> + T - новое окно терминала\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ conda -V\n",
    "\n",
    "$ conda update conda\n",
    "\n",
    "$ conda search \"^python$\"\n",
    "\n",
    "$ conda create -n yourenvname python=x.x anaconda\n",
    "\n",
    "$ source activate yourenvname\n",
    "\n",
    "$ jupyter notebook\n",
    "\n",
    "$ conda install -n yourenvname [package]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Обучение без учителя. Кластеризация.</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задачи машинного обучения\n",
    "* регрессия\n",
    "* классификация\n",
    "* кластеризация\n",
    "\n",
    "#### Обычный порядок действий\n",
    "* получить \"сырые\" данные\n",
    "* понять, что они из себя представляют (см. EDA)\n",
    "* привести их в вид, пригодный для обучения модели\n",
    "* разбить тренировочную выборку на две: обучающую и проверочную (валидационную)\n",
    "* сравнить несколько алгоритмов машинного обучения, их настроек, чтобы получить наилучший результат для P на валидационной выборке\n",
    "* (_возможно_) повторить предыдущие три этапа несколько раз\n",
    "* обучить \"лучшую\" модель на всей тренировочной выборке\n",
    "* использовать модель для получения каких-то результатов\n",
    "* (_возможно_) провести дообучение модели с полученными свежими данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы кластеризации разбивают заанное множество объектов на группы (кластеры) таким образом, \n",
    "чтобы в одном кластере размещались __близкие__ по своим характеристикам объекты, а в разных - __далекие__ объекты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from numpy.random import randn\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Ellipse\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение без учителя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Метод главных компонент__ – понижение размерности входных<br>\n",
    "__Кластеризация__ – разбиение множества объектов на группы на основании анализа признаков этих объектов так, чтобы внутри групп объекты были, в некотором смысле, более \"близкими\" между собой, а вне одной группы – менее \"близкими\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __<font color=\"green\">ВОПРОС?</font>__ \n",
    "Для чего может потребоваться понижение размерности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __<font color=\"blue\">ВАЖНО!</font>__ \n",
    "В реальных задачах кластеризация / понижение размерности и другие алгоритмы обучения без учителя\n",
    "почти всегда используются как один из этапов анализа / оптимизации / сокращения объема обрабатываемы даных,\n",
    "а не как самостоятельное средство анализа."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цели кластеризации:\n",
    "* Упростить дальнейшую обработку данных, разбив множество объектов на группы схожих объектов, чтобы работать с каждой группой по отдельности (_задачи классификации, регрессии, прогнозирования_)\n",
    "* сократить объем хранимых данных, оставив по одному представителью от каждого кластера (_задачи сжатия данных_)\n",
    "* выделить натипичные объекты, которые не подходят ни к одному из кластеров (_задачи одноклассовой классификации_)\n",
    "* Построить иерархию множества объектов (_задача таксономии_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=green>ВОПРОС</font>\n",
    "* Можно ли использовать дерево для регрессии?\n",
    "* Можно ли использовать дерево для кластеризации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод главных компонент (_PCA - Principal Component Analysis_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images11/fig-pca-principal-component-analysis-m.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "https://habr.com/post/304214/ - отличное объяснение\n",
    "https://habr.com/company/ods/blog/325654/ - хорошо разобранный пример для датасета по цветкам ириса\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x = np.arange(0,N)\n",
    "y = 2 * x + np.random.randn(N) * 10 * np.sin((100-x)*np.pi/50 )  * np.abs(N-50) * 0.03\n",
    "X = np.vstack((x,y))\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отцентрируем выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xcentered = (X[0] - x.mean(), X[1] - y.mean())\n",
    "m = (x.mean(), y.mean())\n",
    "plt.scatter(Xcentered [0], Xcentered [1])\n",
    "plt.axvline(0, color=\"grey\")\n",
    "plt.axhline(0, color=\"grey\")\n",
    "print (\"Среднее вектора: \", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</font>\n",
    "* Для чего мы центрируем выборку?<br>\n",
    "* Для чего нам в будущем может потребоваться среднее?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ковариационная матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_range = np.arange(-50,50)\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "\n",
    "def f1(x): return x * 0 + np.random.randn(x.shape[0])* 5\n",
    "def f2(x): return x + np.random.randn(x.shape[0])* 6 * np.sin((x.shape[0]-x)*np.pi/x.shape[0])\n",
    "def f3(x): return -0.5*x - np.random.randn(x.shape[0])* 7 * np.sin((x.shape[0]-x+x.shape[0]/2)*np.pi/x.shape[0])\n",
    "#\n",
    "y1 = f1(x_range)\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.scatter(x_range, y1)\n",
    "plt.axvline(0, color=\"grey\")\n",
    "plt.axhline(0, color=\"grey\")\n",
    "plt.ylim(-50, 50)\n",
    "fig.add_subplot(1, 3,2)\n",
    "#\n",
    "y2 = f2(x_range)\n",
    "plt.scatter(x_range, y2, color=\"green\")\n",
    "plt.axvline(0, color=\"grey\")\n",
    "plt.axhline(0, color=\"grey\")\n",
    "plt.ylim(-50, 50)\n",
    "fig.add_subplot(1, 3,3)\n",
    "#\n",
    "y3 = f3(x_range)\n",
    "plt.scatter(x_range, y3, color=\"red\")\n",
    "plt.axvline(0, color=\"grey\")\n",
    "plt.axhline(0, color=\"grey\")\n",
    "plt.ylim(-50, 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для описания формы случайного вектора нужна ковариационная матрица:<br><br>\n",
    "$$\n",
    "Cov(X_i,X_j) = E[(X_i-E(X_i)) \\cdot  (X_j-E(X_j))] = E(X_i X_j) - E(X_i) \\cdot E(X_j)\n",
    "$$<br>\n",
    "Поскольку в нашем случае $E(X_i) = E(X_j) = 0$, то<br><br>\n",
    "$$\n",
    "Cov(X_i,X_j) = E(X_i X_j) \n",
    "$$<br>\n",
    "Заметим, что если $X_i = X_j$, то <br><br>\n",
    "$$\n",
    "Cov(X_i,X_j) = Var(X_i) \n",
    "$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"green\">ВОПРОС</font>\n",
    "* Что будет на диагонали матрицы?<br>\n",
    "* Что будет, если функция сильно нелинейна?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covmat = np.cov(Xcentered)\n",
    "print (covmat, \"\\n\")\n",
    "print (\"Дисперсия X: \", np.cov(Xcentered)[0,0])\n",
    "print (\"Дисперсия Y: \", np.cov(Xcentered)[1,1])\n",
    "print (\"Ковариация X и Y: \", np.cov(Xcentered)[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Собственные вектора и значения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы получили матрицу, описывающую форму случайной величины. Теперь требуется найти такой вектор, \n",
    "при котором максимизировался бы размер (дисперсия) проекции нашей выборки на него"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проекция единичного вектора $v$ на наш случайный вектор $X$ равна $v^\\intercal X$. В более общей векторной форме (для центрированных величин) дисперсия выражается так:<br><br>\n",
    "$$\n",
    "Var(X) = \\sum = E(X \\cdot X^\\intercal)\n",
    "$$\n",
    "<br>Соответственн, дисперсия проекции:<br><br>\n",
    "$$\n",
    "Var(X^*) = \\sum {^*} = E(X^* \\cdot X^{*\\intercal}) = \\\\\n",
    "    E(( \\overrightarrow{v}^\\intercal X) \\cdot ( \\overrightarrow{v}^\\intercal X)^\\intercal) = \\\\\n",
    "    E( \\overrightarrow{v}^\\intercal X \\cdot \\overrightarrow{v} X^\\intercal) = \\\\\n",
    "    \\overrightarrow{v}^\\intercal \\sum \\overrightarrow{v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко замеить, что дисперсия максимизируется при максимальном значении $v^\\intercal \\sum v$ Существует отношение Рэлея (https://ru.wikipedia.org/wiki/Отношение_Рэлея), которое имеет специальный случай для ковариационных матриц :<br><br>\n",
    "$$\n",
    "R(M, \\overrightarrow{x}^\\intercal) = \\frac { \\overrightarrow{x}^\\intercal M \\overrightarrow{x}}  \n",
    "                                           { \\overrightarrow{x}^\\intercal \\overrightarrow{x}}  =\n",
    "                                           \\lambda \\frac { \\overrightarrow{x}^\\intercal \\overrightarrow{x}}  \n",
    "                                           { \\overrightarrow{x}^\\intercal \\overrightarrow{x}} = \n",
    "                                           \\lambda\n",
    "$$\n",
    "и\n",
    "$$M \\overrightarrow{x} = \\lambda \\overrightarrow{x}$$<br>\n",
    "где $\\overrightarrow{x}$ является собетсенным вектором, а $\\lambda$ - собственным значением"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, направление максимальной дисперсии у проекции всегда совпадает с собственным вектором, \n",
    "имеющим максимальное собственное значение, равное величине этой дисперсии, причем это верно также для проекций на большее число измерений. Ковариационная матрица проекции на $m$-мерное пространство будет максимальна в направлении $m$ собственных векторов, имеющих максимальные собственные значения. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем матрицу ковариации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "covX = np.cov(X)\n",
    "covX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем собственные значения и собственные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_val, eig_vect = np.linalg.eig(covX)\n",
    "print(eig_val)\n",
    "print(eig_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={'aspect': 'equal'}, figsize=(6,6))\n",
    "y3 = f3(x_range)\n",
    "plt.scatter(x_range, y3, color=\"red\")\n",
    "plt.axvline(0, color=\"grey\")\n",
    "plt.axhline(0, color=\"grey\")\n",
    "plt.ylim(-50, 50)\n",
    "\n",
    "x1, y1, x2, y2 = eig_vect.reshape(-1) * 50\n",
    "angle = np.arctan( (y2-y1) / (x1-x2)  ) * 180\n",
    "\n",
    "w, h = np.sqrt(eig_val)*1.5\n",
    "el = Ellipse((0, 0), width=w, height=h,\n",
    "                     angle=angle, linewidth=1, fill=False)  # in data coordinates!\n",
    "ax.add_artist(el)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Снижение размерности (проекция)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наибольший вектор имеет направление, схожее с линией регрессии. Cпроецировав его на него нашу выборку,\n",
    "мы потеряем информацию, сравнимую с суммой остаточных членов регрессии (только расстояние теперь\n",
    "евклидово, а не дельта по Y). В нашем случае зависимость между признаками очень сильная, так что\n",
    "потеря информации будет минимальна. «Цена» проекции — дисперсия по меньшему собственному вектору — как\n",
    "видно из предыдущего графика, очень невелика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Замечание:__ диагональные элементы ковариационной матрицы показывают дисперсии по изначальному базису, \n",
    "    а ее собственные значения – по новому (по главным компонентам)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Замечание:__ На практике, в большинстве случаев, если суммарная потеря информации составляет не более 10-20%,\n",
    "    то можно спокойно снижать размерность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проведения проекции надо выполнить операцию $v^\\intercal X$ (если у нас один вектор). Если же у нас гиперплоскость, то берем матрицу базисных векторов $V^\\intercal X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = -eig_vect[:,1]\n",
    "Xnew = np.dot(v,Xcentered)\n",
    "print (Xnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Восстановление данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас есть вся необходимая информация, а именно координаты базисных векторов в исходном базисе \n",
    "(векторы, на которые мы проецировали) и вектор средних (для отмены центровки). Возьмем, к примеру,\n",
    "наибольшее значение: 94.577 и раскодируем его. Для этого умножим его справа на транспонированный вектор и прибавим вектор средних, или в общем виде для всей выборки: $X^\\intercal v^\\intercal +m$^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10     #номер элемента случайной величины\n",
    "Xrestored = np.dot(Xnew[n],v) + m\n",
    "print ('Восстановленный: ', Xrestored)\n",
    "print ('Оригинальный: ', X[:,n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Сравнение с SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 1)\n",
    "XPCAreduced = pca.fit_transform(X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Our reduced X: \\n', Xnew)\n",
    "print ('Sklearn reduced X: \\n', XPCAreduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">ЗАДАНИЕ</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Загрузите изображения из каталога pictures. \n",
    "* Оставьте последовательно 5, 15 и 30 главных компонент по каждому каналу цветности.\n",
    "* Выведите соответствующие изображение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предполагается, что внутри наших данных имеется какие-то группы. Мы можем захотеть:\n",
    "* выделить эти группы\n",
    "* понять, что их отличает друг от друга\n",
    "* получить возможность сократить врем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images11/cluster_comparison.png\">\n",
    "http://commons.apache.org/proper/commons-math/userguide/ml.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация алгоритмов (https://habr.com/post/101338/)\n",
    "\n",
    "__Иерархические и плоские__<br><br>\n",
    "\n",
    "_Иерархические_ алгоритмы (алгоритмы _таксономии_) строят не одно разбиение выборки на\n",
    "непересекающиеся кластеры, а систему вложенных разбиений. Т.о. на выходе мы получаем дерево кластеров,\n",
    "корнем которого является вся выборка, а листьями — наиболее мелкие кластера.<br><br>\n",
    "_Плоские_ алгоритмы строят одно разбиение объектов на кластеры.\n",
    "<br><br>\n",
    "__Четкие и нечеткие__<br><br>\n",
    "_Четкие_ (или _непересекающиеся_) алгоритмы каждому объекту выборки ставят в соответствие номер кластера,\n",
    "т.е. каждый объект принадлежит только одному кластеру. <br><br>\n",
    "_Нечеткие_ (или _пересекающиеся_) алгоритмы каждому\n",
    "объекту ставят в соответствие набор вещественных значений, показывающих степень отношения объекта к\n",
    "кластерам. Т.е. каждый объект относится к каждому кластеру с некоторой вероятностью.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики расстояний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Евклидово расстояние\n",
    "$$ \\rho (x, x') = \\sqrt{ \\sum_i^m (x_i - x'_i)^2} $$\n",
    "<br>Квадрат евклидова расстояния\n",
    "$$ \\rho (x, x') = \\sum_i^m (x_i - x'_i)^2 $$\n",
    "<br>Манхэттенское расстояние\n",
    "$$ \\rho (x, x') = \\sum_i^m \\mid (x_i - x'_i) \\mid $$\n",
    "<br>Расстояние Чебышева\n",
    "$$ \\rho (x, x') = max ( \\mid x_i - x'_i \\mid ) $$\n",
    "<br>Степенное расстояние\n",
    "$$ \\rho (x, x') = \\sqrt[r]{ \\sum_i^m (x_i - x'_i)^p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм __К-средних (K-means)__ - популярный и простой алгоритм кластеризации, состоящий из следующих шагов:\n",
    "\n",
    "* Выбрать количество кластеров $K$, которые мы считаем целесообразным;\n",
    "* Сгенерировать случайным образом в пространстве поиска $k$ центроидов (точек будущей группировки).\n",
    "* Вычислить для каждой точки набора данных расстояние до центроидов и определить, к какому центроиду она ближе.\n",
    "* Переместить каждый центроид в центр выборки, отнесенной к этому центроиду.\n",
    "* Повторять два последних два шага либо до момента \"схождения\" (обычно это определяется по тому, что их смещение по\n",
    "отношению к предыдущему положению не превышает заранее заданного небольшого значения), либо заданное количество шагов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из https://habr.com/company/ods/blog/325654/ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Начнём с того, что насыпем на плоскость три кластера точек\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.plot(X[:, 0], X[:, 1], 'bo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В scipy есть замечательная функция, которая считает расстояния\n",
    "# между парами точек из двух массивов, подающихся ей на вход\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Прибьём рандомность и насыпем три случайные центроиды для начала\n",
    "np.random.seed(seed=42)\n",
    "centroids = np.random.normal(loc=0.0, scale=1., size=6)\n",
    "centroids = centroids.reshape((3, 2))\n",
    "\n",
    "cent_history = []\n",
    "cent_history.append(centroids)\n",
    "\n",
    "for i in range(3):\n",
    "    # Считаем расстояния от наблюдений до центроид\n",
    "    distances = cdist(X, centroids)\n",
    "    # Смотрим, до какой центроиде каждой точке ближе всего\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    # Положим в каждую новую центроиду геометрический центр её точек\n",
    "    centroids = centroids.copy()\n",
    "    centroids[0, :] = np.mean(X[labels == 0, :], axis=0)\n",
    "    centroids[1, :] = np.mean(X[labels == 1, :], axis=0)\n",
    "    centroids[2, :] = np.mean(X[labels == 2, :], axis=0)\n",
    "\n",
    "    cent_history.append(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# А теперь нарисуем всю эту красоту\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(4):\n",
    "    distances = cdist(X, cent_history[i])\n",
    "    labels = distances.argmin(axis=1)\n",
    "\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.plot(X[labels == 0, 0], X[labels == 0, 1], 'bo', label='cluster #1')\n",
    "    plt.plot(X[labels == 1, 0], X[labels == 1, 1], 'co', label='cluster #2')\n",
    "    plt.plot(X[labels == 2, 0], X[labels == 2, 1], 'mo', label='cluster #3')\n",
    "    plt.plot(cent_history[i][:, 0], cent_history[i][:, 1], 'rX')\n",
    "    plt.legend(loc=0)\n",
    "    plt.title('Step {:}'.format(i + 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбор числа кластеров для kMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В основе (обычно) - здравый смысл. Однако для K-means часто используют такую метрику:\n",
    "$$\n",
    "J(C)= \\sum_{k=1}^K \\sum_{i \\in C_k} \\mid \\mid x_i - \\mu _k \\mid \\mid ^2 \\rightarrow min\n",
    "$$\n",
    "где $C$ - множество кластеров мощности $K$, $\\mu _k$ - центроид кластера $C_k$\n",
    "<br><br>Часто выбирают число кластеров, начиная с которого функционал $J(C)$ падает \"не так быстро\"<br><br>\n",
    "$$\n",
    "D(K) = \\frac {J(C_k)-J(C_{k+1})} {J(C_{k-1})-J(C_{k})}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Агломеративная кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Начальное разбиение - каждая точка является центроидом кластера, состоящего только из нее\n",
    "* Сортируем попарные расстояния между центрами кластеров по возрастанию\n",
    "* Берём пару ближайших кластеров, объединяем их в один и пересчитываем центр кластера\n",
    "* повторяем п. 2 и 3 до тех пор, пока все данные не склеятся в один кластер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существуют различные методы объединения точек:\n",
    "* Single linkage — минимум попарных расстояний между точками из двух кластеров<br><br>\n",
    "$$\n",
    "d(C_i, C_j) = min_{x \\in C_i, x_j \\in C_j} \\mid \\mid x_i - x_j \\mid \\mid\n",
    "$$<br>\n",
    "* Complete linkage — максимум попарных расстояний между точками из двух кластеров<br><br>\n",
    "$$\n",
    "d(C_i, C_j) = max_{x \\in C_i, x_j \\in C_j} \\mid \\mid x_i - x_j \\mid \\mid\n",
    "$$<br>\n",
    "* Average linkage — среднее попарных расстояний между точками из двух кластеров<br><br>\n",
    "$$\n",
    "d(C_i, C_j) = \\frac{1}{n_i n_j} \\sum_{x \\in C_i} \\sum_{x_j \\in C_j} \\mid \\mid x_i - x_j \\mid \\mid\n",
    "$$<br>\n",
    "* Centroid linkage — расстояние между центроидами двух кластеров<br><br>\n",
    "$$\n",
    "d(C_i, C_j) = \\mid \\mid \\mu _i - \\mu _j \\mid \\mid\n",
    "$$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример из https://habr.com/company/ods/blog/325654/:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "X = np.zeros((150, 2))\n",
    "\n",
    "np.random.seed(seed=42)\n",
    "X[:50, 0] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "X[:50, 1] = np.random.normal(loc=0.0, scale=.3, size=50)\n",
    "\n",
    "X[50:100, 0] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "X[50:100, 1] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "\n",
    "X[100:150, 0] = np.random.normal(loc=-1.0, scale=.2, size=50)\n",
    "X[100:150, 1] = np.random.normal(loc=2.0, scale=.5, size=50)\n",
    "\n",
    "distance_mat = pdist(X) # pdist посчитает нам верхний треугольник матрицы попарных расстояний\n",
    "\n",
    "Z = hierarchy.linkage(distance_mat, 'single') # linkage — реализация агломеративного алгоритма\n",
    "plt.figure(figsize=(10, 5))\n",
    "dn = hierarchy.dendrogram(Z, color_threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity propagation (\"Распространение близости\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/auto_examples/cluster/plot_affinity_propagation.html#sphx-glr-auto-examples-cluster-plot-affinity-propagation-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Получает на вход матрицу схожести между элементами датасета $S = N x N$\n",
    "* Возвращает набор меток, присвоенных этим элементам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.5,\n",
    "                            random_state=0)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute Affinity Propagation\n",
    "af = AffinityPropagation(preference=-50).fit(X)\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "labels = af.labels_\n",
    "\n",
    "n_clusters_ = len(cluster_centers_indices)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels,\n",
    "                                           average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels, metric='sqeuclidean'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "from itertools import cycle\n",
    "\n",
    "plt.close('all')\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    class_members = labels == k\n",
    "    cluster_center = X[cluster_centers_indices[k]]\n",
    "    plt.plot(X[class_members, 0], X[class_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=14)\n",
    "    for x in X[class_members]:\n",
    "        plt.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">ЗАДАНИЕ</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем MNIST\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "mnist = load_digits()\n",
    "X = mnist.data / 255.0\n",
    "y = mnist.target\n",
    "\n",
    "print (X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из проекции на плоскость (2 основных компоненты) видно, что имеются явно выраженные кластеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print('Projecting %d-dimensional data to 2D' % X.shape[1])\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, \n",
    "            edgecolor='none', alpha=0.7, s=40,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "plt.colorbar()\n",
    "plt.title('MNIST. PCA projection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Обучите ансамбль из 20 случайных деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Напишите функцию, сокращающую размерность входных чисел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Постепенно увеличивая размерность от 1, добейтесь того, чтобы полученная Вами точность отличалась от\n",
    "точности на полных изображениях не более, чем на:\n",
    "    - 10%\n",
    "    - 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Постройте график зависимости точности от количества главных компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экзаменационные вопросы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* обучение без учителя\n",
    "* метод главных компонент\n",
    "* методы кластеризации"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
